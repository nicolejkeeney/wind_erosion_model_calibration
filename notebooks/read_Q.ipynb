{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0424460",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"read_Q.ipynb \n",
    "\n",
    "    Read in Q flux data from csv file that has combined data for all new (non NEAT) sites \n",
    "    \n",
    "    Author: Nicole Keeney \n",
    "    Date Created: 08-20-2021 \n",
    "    Modification History: n/a\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd \n",
    "import os \n",
    "import sys\n",
    "\n",
    "# Import read_datetime_csv function from read_data module in the parent directory\n",
    "sys.path.append('..')\n",
    "from utils.read_data_utils import read_datetime_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20db64c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_site_flux(fluxPath, site_name): \n",
    "    \"\"\" Read in sediment flux data for site of interest. Convert date to datetime \n",
    "    \n",
    "    Args: \n",
    "        fluxPath (str): path to csv file containing the combined flux data for Pullman, Moab, Mandan, SanLuisValley, and Holloman\n",
    "        site_name (str): name of site. Must match folder and filename (choose from: Pullman, Moab, Mandan, SanLuisValley, Holloman)\n",
    "    \n",
    "    Returns: \n",
    "        flux_df (pd.DataFrame): data \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    flux_df = read_datetime_csv(path=fluxPath, date_col=\"Date\", strftime=\"%Y%m%d\") # Read in file \n",
    "    flux_df = flux_df.replace(to_replace=\"San Luis Valley\",value=\"SanLuisValley\",regex=True) # Remove spaces to match file conventions in repository\n",
    "    flux_df = flux_df[flux_df[\"Site\"].str.contains(site_name)] # Grab data for site of interest \n",
    "    return flux_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6311500e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- Input site name of interest -------------------\n",
    "# You need to run this cell for each site \n",
    "\n",
    "site_name = \"Moab\"\n",
    "#site_name = \"Pullman\"\n",
    "#site_name = \"Holloman\"\n",
    "#site_name = \"Mandan\"\n",
    "#site_name = \"SanLuisValley\"\n",
    "\n",
    "\n",
    "# ------------------- Set and check directory and filepaths -------------------\n",
    "\n",
    "data_dir = \"../data/site_data/\" # Main directory containing site data \n",
    "fluxPath = data_dir+\"site_data_combined/HT_Flux_results.csv\" # Flux data \n",
    "output_dir = data_dir+site_name+\"/field_Q/\" # Output dir to store results\n",
    "\n",
    "for directory in data_dir, output_dir: # Raise error if directory does not exist\n",
    "    if os.path.isdir(directory) == False: \n",
    "        raise ValueError(\"Directory does not exist: \"+directory)\n",
    "    else: \n",
    "        pass # Do nothing\n",
    "\n",
    "if os.path.isfile(fluxPath) == False: \n",
    "    raise ValueError(\"File does not exist: \"+filepath)\n",
    "else: \n",
    "    pass # Do nothing\n",
    "    \n",
    "    \n",
    "# ------------------- Read in data and save to drive -------------------\n",
    "\n",
    "# Read in data \n",
    "flux_df = read_site_flux(fluxPath=fluxPath, site_name=site_name)\n",
    "\n",
    "# Grab just sediment flux data \n",
    "flux_df = flux_df.rename(columns={\"Mean\":\"Q_field\"})\n",
    "Q_field = flux_df[\"Q_field\"]\n",
    "\n",
    "# Save to drive \n",
    "Q_field.to_csv(output_dir+\"/\"+site_name+\"_fieldQ.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
